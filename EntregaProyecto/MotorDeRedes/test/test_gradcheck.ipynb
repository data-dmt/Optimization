{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb51e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from src.NeuralNetwork import NeuralNetwork\n",
    "from src.Losses import mean_squared_error\n",
    "\n",
    "X = np.array([[0.0, 1.0],\n",
    "              [1.0, 0.0]])\n",
    "Y = np.array([[1.0, 0.0]])\n",
    "\n",
    "model = NeuralNetwork(n_input=2, n_hidden=2, n_output=1, activation='sigmoid')\n",
    "Y_hat, cache = model.forward(X)\n",
    "grads = model.backward(X, Y, cache)\n",
    "epsilon = 1e-5\n",
    "numerical_grads = {}\n",
    "\n",
    "for name, param in model.params.items():\n",
    "    grad_approx = np.zeros_like(param)\n",
    "    for i in np.ndindex(param.shape):\n",
    "        original_value = param[i]\n",
    "        param[i] = original_value + epsilon\n",
    "        y_hat_plus, _ = model.forward(X)\n",
    "        J_plus = mean_squared_error(Y, y_hat_plus)\n",
    "        param[i] = original_value - epsilon\n",
    "        y_hat_minus, _ = model.forward(X)\n",
    "        J_minus = mean_squared_error(Y, y_hat_minus)\n",
    "        grad_approx[i] = (J_plus - J_minus) / (2 * epsilon)\n",
    "        param[i] = original_value\n",
    "        \n",
    "    numerical_grads[name] = grad_approx\n",
    "\n",
    "for name in grads.keys():\n",
    "    diff = np.linalg.norm(grads[name] - numerical_grads[name]) / (\n",
    "        np.linalg.norm(grads[name]) + np.linalg.norm(numerical_grads[name]) + 1e-8\n",
    "    )\n",
    "    print(f\"{name}: diferencia relativa = {diff:.8f}\")\n",
    "    assert diff < 1e-4, f\"Gradiente incorrecto en {name}\"\n",
    "\n",
    "print(\"\\nTodos los gradientes son correctos (diferencia < 1e-4)\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
